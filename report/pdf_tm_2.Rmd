---
title: "Inventory"
output: html_document
date: "2023-12-05"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Output of webscaping as of DECEMBER 2023 - JEFCA
The process commences with web scraping data from the JECFA portal, resulting in a dataset in csv format.
Subsequently, data pre-processing is initiated to extract valuable information from both URLs and strings. Notably, experts have singled out the 'Tox.Monograph' field as the most dependable source of information.

![*Variables in the csv to capture fields in the webpage*](jecfa1.png)
![*Variables in the csv to capture fields in the webpage*](jecfa2.png)


In an initial endeavor, an approach was made to directly access documents through the 'Tox.Monograph' links. Unfortunately, this method posed significant challenges due to the data's inherent heterogeneity. These links primarily led to HTML pages, lacking direct access to the desired PDF documents, as elucidated in Table 1.

As a strategic shift, the code now centers its efforts on discerning the 'Tox.Monograph' names within the context of Food Additives Series (FAS), as exemplified in Table 2. This approach is designed to facilitate efficient access to crucial information within the 'Tox.Monograph' field, enhancing its usability for subsequent steps.
Although almost all 85 FAS codes are referenced, the ones collectively accounting for 50% of the records are: 40, 42, 44, 46, 48, 50, 54 and 59.
We accessed and stored all the FAS from https://www.who.int/groups/joint-fao-who-expert-committee-on-food-additives-(jecfa)/publications/toxicological-monographs in pdf format, whenever available, or HTML as the 1st, 4th, 5th, 6th, 8th, 10th, and 12th through 52nd series of FAS monographs are available in HTML format only. WHO monographs beginning with the 51st series are also available in PDF format.

```{r, eval=T, cache=F, echo=F, warning=FALSE, message=FALSE}

library(gtsummary)
library(dplyr)

setwd("~/Efsa_pdf_tm/Jecfa_web")
jecfa <- readRDS("jecfa2.rds")

DT::datatable(jecfa, 
              rownames = FALSE,
              filter = list(position = 'top'),
              caption = "Overview of jecfa.csv")


# Identify the type of document
jecfa$type <- substr(jecfa$Tox.Monograph_sourcelink, nchar(jecfa$Tox.Monograph_sourcelink) - 3, nchar(jecfa$Tox.Monograph_sourcelink))

jecfa$type <- ifelse(jecfa$type %in% c(".pdf", ".htm", "html"), jecfa$type, "other")

# Identify the host
URL_parts <- function(x) {
    m <- regexec("^(([^:]+)://)?([^:/]+)(:([0-9]+))?(/.*)", x)
    parts <- do.call(rbind,
                     lapply(regmatches(x, m), `[`, c(3L, 4L, 6L, 7L)))
    colnames(parts) <- c("protocol","host","port","path")
    parts
}

jecfa$host <- URL_parts(jecfa$Tox.Monograph_sourcelink)[ ,2]

# Function to extract parts of a string
extract_initial_part <- function(input_string) {
  split_string <- unlist(strsplit(input_string, "-"))
  initial_part <- split_string[1]
}

extract_initial_partII <- function(input_string) {
  split_string <- unlist(strsplit(input_string, "/"))
  initial_part <- split_string[1]
}

extract_initial_partIII <- function(input_string) {
  split_string <- unlist(strsplit(input_string, "JECFA"))
  initial_part <- split_string[1]
}

# Apply the functions to each element of the vector
jecfa$Tox_monograph_abbr <- sapply(
                                 sapply(
                                   sapply(jecfa$Tox.Monograph, extract_initial_part), 
                                   extract_initial_partII), 
                                  extract_initial_partIII) 

jecfa$Tox_monograph_abbr <- ifelse(substr(jecfa$Tox_monograph_abbr, 1, 3) == "See", 
                                   substr(jecfa$Tox_monograph_abbr, 5, nchar(jecfa$Tox_monograph_abbr)),    
                                   jecfa$Tox_monograph_abbr )

jecfa$Tox_monograph_abbr <- ifelse(substr(jecfa$Tox_monograph_abbr, 1, 2) == '">', 
                                   substr(jecfa$Tox_monograph_abbr, 3, nchar(jecfa$Tox_monograph_abbr)),    
                                   jecfa$Tox_monograph_abbr )

jecfa$Tox_monograph_abbr <- ifelse(substr(jecfa$Tox_monograph_abbr, 1, 3) != 'FAS' & 
                                      substr(jecfa$Tox_monograph_abbr, 1, 12) != "NOT PREPARED", 
                                   "NOT A FAS",    
                                   jecfa$Tox_monograph_abbr )

tbl1 <- 
      jecfa %>% 
      select(host, type) %>%
      tbl_summary( missing = "always",
                   by = type,
#                   label = list(host ~ "Link to Tox Monograph"),
                   statistic = list(host ~ "{n} ({p})"),
                   sort = all_categorical() ~ "frequency"
                   ) %>%
    modify_table_styling(
    columns = label,
    rows = label == "Unknown",
    footnote = "corrupt link such as /food-additives-contaminants-jecfa-database/Document/Index/989"
                        ) %>%
    modify_caption("**Table 1. Description of the link to the monograph**")

tbl1 

tbl2 <- 
      jecfa %>% 
     # filter(Tox.Monograph_sourcelink == "http://www.inchem.org/pages/jecfa.html") %>% 
#      select(Tox.Monograph, Chemical.Names) %>% 
      select(Tox_monograph_abbr) %>% 

      tbl_summary( missing = "always",
      #            label = list(Tox.Monograph_sourcelink ~ "Link"),
                   sort = all_categorical() ~ "alphanumeric",
                   ) %>%
     modify_table_styling(
        columns = label,
           rows = label == "NOT A FAS",
    footnote = "Used by authors for content that does not conform to a FAS format"
                        ) %>%
    modify_caption("**Table 2. Distribution of pre-processed Tox.Monograph variable**")


tbl2 

tbl3 <- 
      jecfa %>% 
      filter(Tox_monograph_abbr %in% c("NOT A FAS","NOT PREPARED", NA)) %>% 
      select(host, Tox.Monograph_sourcelink, Tox_monograph_abbr, type) %>% 
            tbl_summary( missing = "always",
      #            label = list(Tox.Monograph_sourcelink ~ "Link"),
                   sort = all_categorical() ~ "alphanumeric",
                   by = type
                   ) %>%
    modify_caption("**Table 3. Distribution of strange cases**")


tbl3

tbl4 <- 
      jecfa %>% 
      filter(Tox_monograph_abbr %in% c("NOT PREPARED", "NOT A FAS", NA)) %>% 
      select(Report_sourcelink, Report) %>% 
            tbl_summary( missing = "no",
      #            label = list(Tox.Monograph_sourcelink ~ "Link"),
                   sort = all_categorical() ~ "alphanumeric",
                                      ) %>%
    modify_caption("**Table 4. Distribution of strange cases**")


tbl4
```



```{r, eval=F, echo=F, cache=T, message=FALSE}
library(pdftools)
pdf_url <- "https://apps.who.int/iris/bitstream/10665/40951/1/WHO_TRS_488.pdf"
local_path <- "C://Users/Ileana/Documents/pdf_tm/WHO_TRS_488.pdf"
# Download the PDF file
download.file(pdf_url, destfile = local_path, mode = "wb")


# Report corresponding to https://www.who.int/publications/i/item/9789240068414

local_path <- "C://Users/Ileana/Documents/pdf_tm/9789240068414-eng.pdf"
# Check if the file was downloaded successfully
if (file.exists(local_path)) {

# Extract text from the PDF
suppressMessages({pdf_text <- pdf_ocr_text(local_path)})
pdf_text_pre <- tolower(pdf_text)

# Search for a specific text in the extracted text
keywords <- c("dose-response", "dose response", "modelling", "modeling", "bmd", "bmr", "benchmark dose")
matching_pages <- vector("list", length(keywords))

 for (i in seq_along(keywords)) {
    matching_pages[[i]] <- grep(keywords[i], pdf_text_pre)
 }

for (i in seq_along(keywords)) {
    cat("The text '", keywords[i], "' was found on the following pages:\n")
    cat(matching_pages[[i]], sep = ", ")
    cat("\n\n")
  }
} else {
  cat("Failed to download the PDF file.")
}

```

## Download the TRS - (when a FAS is not available) from Report_sourcelink present in jecfa.csv after pre-processing the Report name (n=667)

```{r, eval=F, echo=F, cache=T, message=FALSE}
# Load required libraries
library(httr)

jecfa$Report_clean <- ifelse(substr(jecfa$Report, 1, 2) == '">', 
                                   substr(jecfa$Report_clean, 3, nchar(jecfa$Report_clean)),    
                                   jecfa$Report)

jecfa$Report_clean <-ifelse(substr(jecfa$Report_clean, 1, 3) %in% c("See","see"), 
                                   substr(jecfa$Report_clean, 5, nchar(jecfa$Report_clean)),    
                                   jecfa$Report_clean)


# Function to sanitize a string for use as a file name
sanitize_file_name <- function(file_name) {
  # Replace characters not allowed in file names with underscores
  file_name <- gsub("[^[:alnum:]_\\.-]", "_", file_name, perl = TRUE)
  
  # Remove leading and trailing underscores
  file_name <- gsub("^_+|_+$", "", file_name)
  
}

# Create the "TRS" folder 
if (!dir.exists("TRS")) {
  dir.create("TRS")
}

pdf_urls <- jecfa$Report_sourcelink

# Loop through the URLs and download the PDFs into the "TRS" folder
for (i in 11:length(pdf_urls)) {
  url <- pdf_urls[i]
  prefix <- as.character(i)  # Prefix with the position in the original JECFA database
  
  if (
    !is.na(jecfa$Tox_monograph_abbr[i]) &
      !(jecfa$Tox_monograph_abbr[i] %in% c("NOT A FAS")) &
      !(substr(jecfa$Tox_monograph_abbr, 1, 12)[i] %in% c("NOT PREPARED"))) {
    cat("Condition not satisfied, skipping...\n")
    next  # Skip this iteration and move to the next URL
  }
  
  # Check if the URL is missing or empty
  if (is.na(url) || url == "") {
    cat("URL missing or empty, skipping...\n")
    next  # Skip this iteration and move to the ntable(ext URL
  }
  
  # Check if the URL has a valid format
  if (!grepl("^https?://", url, ignore.case = TRUE)) {
    cat("Invalid URL format, skipping:", url, "\n")
    next  # Skip this iteration and move to the next URL
  }
  
  # Create the file name with prefix as RefID number
  file_name <- paste0("TRS/", prefix, "-", substr(sanitize_file_name(jecfa$Report_clean)[i],1, 7),".pdf")

  # Check if the file already exists in the "TRS" folder, and if not, download it
  if (!file.exists(file_name)) {
    response <- GET(url)
    
    if (status_code(response) == 200) {
      # Save the PDF with the prefix in the "TRS" folder
      writeBin(content(response, "raw"), file_name)
      cat("Downloaded:", file_name, "\n")
    } else {
      cat("Failed to download:", url, "\n")
    }
  } else {
    cat("File already exists:", file_name, "\n")
  }
}

length(list.files("TRS"))
length(list.files("TRS_Unique"))
```
Here's a breakdown of what the code is doing:

1. Cleaning the "Report" column in the dataset:

The code cleans the "Report" column in the dataset (jecfa$Report_clean) by removing unwanted characters at the beginning of the string. If the string starts with '">', it removes the first two characters. This step aims at handling formatting issues in the data.

2. Sanitizing file names:

The code defines a function that replaces characters that are not allowed in file names with underscores. It also removes leading and trailing underscores from the file name.

3. Creating a folder named "TRS":

If the "TRS" folder does not exist in the current directory, the code creates it. This folder is intended to store downloaded PDF files.

4. Extracting PDF URLs:

The code extracts a vector of PDF URLs from the "Report_sourcelink" column in the dataset and assigns it to the variable pdf_urls.

5. Downloading PDFs:

The code enters a loop to process each URL in the pdf_urls vector.
It checks various conditions before attempting to download a PDF:
It skips URLs associated with specific conditions based on values in the dataset (e.g., Monograph "NOT PREPARED" or other conditions).
It checks if the URL is missing or empty and skips such cases.
It verifies that the URL has a valid format (starting with "http://" or "https://").
For URLs that meet the conditions, the code constructs a file name for the PDF file. The file name includes a prefix based on the position in the original dataset and a sanitized version of the "Report_clean" column.
It checks if the PDF file already exists in the "TRS" folder. If not, it proceeds to download the PDF file using the GET function from the httr library.
If the download is successful (status code 200), it saves the PDF file in the "TRS" folder with the constructed file name.
If the download fails or if the file already exists, it logs appropriate messages.

We identified 56 unique PDFs within TRS folder. 

```{r, eval=F, echo=F, cache=F, message=FALSE}
## Identify unique PDFs within TRS folder by converting their name from RefID-xxx_yyy into xxx_yyy.pdf format

# Set the path to the folder containing the Unique PDF files 
if (!dir.exists("TRS_Unique")) {
  dir.create("TRS_Unique")
}

file.copy(from =  list.files("TRS", full.names = T), to = "TRS_Unique")

# Set the path to the folder containing the PDF files
pdf_folder <- "TRS_Unique"

# List all files in the folder
pdf_files <- list.files(path = pdf_folder, pattern = "*.pdf", full.names = TRUE)

# Loop through each PDF file
for (pdf_file in pdf_files) {
  # Extract the current file name
  current_file_name <- basename(pdf_file)
  
  # Split the file name by "-" to get the part after the last "-"
  file_name_parts <- strsplit(current_file_name, "-")[[1]]
  
  # Check if there are at least two parts (xxx-yy.pdf format)
  if (length(file_name_parts) >= 2) {
    # Extract the last part (yy.pdf) and create the new file name
    new_file_name <- file_name_parts[length(file_name_parts)]
    
    # Generate the full path for the new file name
    new_pdf_file <- file.path(pdf_folder, new_file_name)
    
    # Rename the file
    file.rename(pdf_file, new_pdf_file)
    
    # Print a message indicating the renaming
    cat("Renamed:", current_file_name, "to", new_file_name, "\n")
  }
}

# List the files in the folder after renaming
cat("Files in the folder after renaming:\n")
print(list.files(path = pdf_folder))
```

## Text mining on TRS (when a FAS is not available)

```{r, eval=F, cache=F, echo=F, warning=FALSE, message=FALSE}

library(tesseract)
library(pdftools)
library(stringdist)

# Functions to call

extract_initial_part <- function(input_string) {
  split_string <- unlist(strsplit(input_string, "-"))
  initial_part <- split_string[1]
}

# Specify the folder containing PDF files

pdf_folder <- "TRS_Unique"

# List all PDF files in the folder
pdf_files <- list.files(path = pdf_folder, pattern = "*.pdf", full.names = TRUE)

# Initialize or access results_list
if (file.exists("results_list.rds")) {
  results_list <- readRDS("results_list.rds")
  final_results_df_temp <- do.call(rbind, results_list)

} else {
  results_list <- list()
}

# Define the keywords to search for
keywords <- c("dose-response", "dose response", "modelling", "modeling", "bmd", "bmr", "benchmark dose", "benchmark-dose")

# Initialize an empty data frame to store the final results
final_results_df <- data.frame(File = character(0), stringsAsFactors = FALSE)

# Initialize variables for each keyword to store the list of pages
for (keyword in keywords) {
  assign(keyword, list())
}


# DELETE TRS_102! 

# Loop through each PDF file in the folder
for (pdf_file in pdf_files) {
  # Check if the file exists
  if (file.exists(pdf_file) & !(basename(pdf_file) %in% final_results_df_temp[,1])) {
    # Extract text from the PDF
    suppressMessages({pdf_text <- pdf_ocr_text(pdf_file)})
    pdf_text_pre <- tolower(pdf_text)
    
    # Initialize a list to store matching pages for each keyword
    matching_pages <- vector("list", length(keywords))

    # Search for each keyword in the PDF text
    for (i in seq_along(keywords)) {
      matching_pages[[i]] <- grep(keywords[i], pdf_text_pre)
    }

    # Store the results for this PDF in the list
    pdf_results_list <- list(File = basename(pdf_file))
    for (i in seq_along(keywords)) {
      keyword <- keywords[i]
      pdf_results_list[[keyword]] <- matching_pages[[i]]
    }
    
    results_list[[pdf_file]] <- pdf_results_list
    saveRDS(results_list, file = "~/Efsa_pdf_tm/Jecfa_web/results_list.rds")

  } else {
    cat("Failed to find the PDF file:", pdf_file, "\n")
  }
}

# Convert the results list to a data frame with one row per PDF
final_results_df <- do.call(rbind, results_list)
```

This code will create a data frame with one row per PDF file, and each column corresponds to a keyword and its list of page matches. The "File" column contains the file name of the PDF.
In details:

1. Extract Text from PDFs: The code uses the pdf_ocr_text function from the pdftools library to extract text from each PDF file. The extracted text is converted to lowercase for consistent searching.

2. Search for Keywords: For each PDF file, the code searches for predefined keywords related to the content of interest, such as "dose-response," "modelling," "bmd", "bmr" and "benchmark-dose" and some variations of these. It records the pages in which these keywords are found within the PDF.

3. Store Results: The results for each PDF file, including the file name and the list of pages where keywords are found, are stored in a list. The list is indexed by the PDF file name.

4. Final Data Frame: Lastly, the code combines the individual results from each PDF file into a final data frame. 

Overall, the code automates the extraction of specific information from a collection of PDF files, making it easier to identify which PDFs contain relevant content based on predefined keywords.

It follows a **Match between JECFA refID (i.e., row names) and the TRS pdf**. 

```{r, eval=F, cache=F, echo=F, warning=FALSE, message=FALSE}

# Set the path to the folder containing the PDF files
pdf_folder <- "~/Efsa_pdf_tm/TRS"

# List all files in the folder
pdf_files <- list.files(path = pdf_folder, pattern = "*.pdf", full.names = TRUE)

# Initialize
RefID_vector <- numeric()
File_vector <- character()

# Loop through each PDF file
for (pdf_file in pdf_files) {

  # Extract the current file name
  current_file_name <- basename(pdf_file)
  
  # Split the file name by "-" to get the part after the last "-"
  RefID <- as.numeric(strsplit(current_file_name, "-")[[1]][1])
  filename <- strsplit(current_file_name, "-")[[1]][2]
  
  # Store the information 
   RefID_vector <- c(RefID_vector, RefID)
   File_vector <- c(File_vector, filename)
}

# Create a data frame from the vectors
maptojecfa <- data.frame(RefID = RefID_vector, File = File_vector)


results_list <- readRDS("~/Efsa_pdf_tm/Jecfa_web/results_list.rds")
final_results_df <- data.frame(do.call(rbind, results_list))

maptoJecfa<-merge(maptojecfa, final_results_df, by="File")
#saveRDS(maptoJecfa, file = "maptoJecfa.rds")

df <- as.data.frame(lapply(maptoJecfa, as.character), stringsAsFactors = FALSE)
#write.csv(df, "TRSmaptoJecfa.csv")

```

## Text mining on FAS (n=73 unique FAS)
The same procedure described for text mining TRS is applied to FAS. The only difference is in the way the pdfs are accessed.

```{r, eval=F, cache=F, echo=F, warning=FALSE, message=FALSE}

library(tesseract)
library(pdftools)
library(stringdist)

# Functions to call

extract_initial_part <- function(input_string) {
  split_string <- unlist(strsplit(input_string, "-"))
  initial_part <- split_string[1]
}

# Specify the folder containing PDF files

pdf_folder <- "~/Efsa_pdf_tm/FAS/jecfa_files"


# List all PDF files in the folder
pdf_files <- list.files(path = pdf_folder, pattern = "*.pdf", full.names = TRUE)

# Initialize or access results_list
if (file.exists("results_list1.rds")) {
  results_list1 <- readRDS("results_list1.rds")
  final_results_df_temp <- do.call(rbind, results_list1)

} else {
  results_list1 <- list()
}

# Define the keywords to search for
keywords <- c("dose-response", "dose response", "modelling", "modeling", "bmd", "bmr", "benchmark dose", "benchmark-dose")

# Initialize an empty data frame to store the final results
final_results_df1 <- data.frame(File = character(0), stringsAsFactors = FALSE)

# Initialize variables for each keyword to store the list of pages
for (keyword in keywords) {
  assign(keyword, list())
}



# Loop through each PDF file in the folder
for (pdf_file in pdf_files) {
  # Check if the file exists
  if (file.exists(pdf_file) & !(basename(pdf_file) %in% final_results_df_temp[,1])) {
#   if (file.exists(pdf_file)) {
    # Extract text from the PDF
    suppressMessages({pdf_text <- pdf_ocr_text(pdf_file)})
    pdf_text_pre <- tolower(pdf_text)
    
    # Initialize a list to store matching pages for each keyword
    matching_pages <- vector("list", length(keywords))

    # Search for each keyword in the PDF text
    for (i in seq_along(keywords)) {
      matching_pages[[i]] <- grep(keywords[i], pdf_text_pre)
    }

    # Store the results for this PDF in the list
    pdf_results_list1 <- list(File = basename(pdf_file))
    for (i in seq_along(keywords)) {
      keyword <- keywords[i]
      pdf_results_list1[[keyword]] <- matching_pages[[i]]
    }
    
    results_list1[[pdf_file]] <- pdf_results_list1
#    saveRDS(results_list1, file = "results_list1.rds")

  } else {
    cat("Failed to find the PDF file:", pdf_file, "\n")
  }
}

# Convert the results list to a data frame with one row per PDF
final_results_df1 <- data.frame(do.call(rbind, results_list1))
```

It follows a **match between JECFA refID and the FAS pdf **

```{r, eval=F, cache=F, echo=F, warning=FALSE, message=FALSE}

# Prepare to link by FAS field and keeping the original row position in jecfa.csv
jecfa$FAS<- ifelse(nchar(jecfa$FAS)>3, substr(jecfa$FAS,1,2), jecfa$FAS)

# Some ad hoc adjustment to link jecfa with FAS 74, FAS 60 and FAS 27
jecfa$FAS<- ifelse(jecfa$FAS=="" & substr(jecfa$Tox_monograph_abbr,1,6) == "FAS 74","74", jecfa$FAS)
jecfa$FAS<- ifelse(jecfa$FAS=="" & substr(jecfa$Tox_monograph_abbr,1,6) == "FAS 60","60", jecfa$FAS)
jecfa$FAS<- ifelse(jecfa$FAS=="" & substr(jecfa$Tox_monograph_abbr,1,6) == "FAS 27","27", jecfa$FAS)

jecfa$RefID<-row.names(jecfa)

results_list1 <- readRDS("results_list1.rds")
final_results_df1 <- data.frame(do.call(rbind, results_list1))

# Standardize some FAS contents
final_results_df1$FAS<-ifelse(nchar(final_results_df1$File)>5, substr(final_results_df1$File,1,2), substr(final_results_df1$File,1,1))


maptoJecfa1<-merge(jecfa[c("RefID","FAS")], final_results_df1, by = "FAS")
#saveRDS(maptoJecfa1, file = "maptoJecfa1.rds")

df1 <- as.data.frame(lapply(maptoJecfa1, as.character), stringsAsFactors = FALSE)
df1$source_TM<-"FAS"
df$source_TM<-"TRS"
df1 <- df1[, match(names(df), names(df1))]

# write.csv(df1, "FASmaptoJecfa.csv")
jecfa_TM<-rbind(df,df1)

# Replace integer(0) which means no words found as missing
jecfa_TM[] <- lapply(jecfa_TM, function(x) ifelse(x == "integer(0)", NA, x))

# Dataset containing the results of text mining on FAS and TRS
saveRDS(jecfa_TM, file = "~/Efsa_pdf_tm/Jecfa_web/jecfa_TM.rds")
# write.csv2(jecfa_TM, "jecfa_TM.csv")

# Dataset containing the results of text mining on FAS and TRS plus original jecfa.csv variables
maptoJecfa_all<-merge(jecfa, jecfa_TM, by = "RefID", all.x = T) %>% distinct()
saveRDS(maptoJecfa_all, file = "~/Efsa_pdf_tm/Jecfa_web/maptoJecfa_all.rds")

#write.csv2(maptoJecfa_all, "maptoJecfa_all.csv")
```

## Description of jecfa records with text mining on FAS or TRS
```{r, eval=T, cache=F, echo=F, warning=FALSE, message=FALSE}

#library(VennDiagram)
library(dplyr)
library(gtsummary)

jecfa_TM<- readRDS("~/Efsa_pdf_tm/Jecfa_web/jecfa_TM.rds")

# Convert page matches with a 1/0 variable
jecfa_TM[, 3:10]<-as.integer(jecfa_TM[, 3:10] != 0)
jecfa_TM[is.na(jecfa_TM)] <- 0

tbl5 <- 
      jecfa_TM %>% 
      mutate(dose_response = dose.response|dose.response.1, 
             modelling = modelling | modeling, 
             benchmark_dose = benchmark.dose | benchmark.dose.1) %>% 
      select(dose_response, 
             modelling, 
             bmd, 
             bmr, 
             benchmark_dose,
             source_TM) %>% 
      tbl_summary( missing = "no",
                   percent = "col",
                  label = list(dose_response ~ "dose response", benchmark_dose ~ "benchmark dose"),
                   sort = all_categorical() ~ "alphanumeric",
      by= source_TM
                   )%>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "dose_response",
     footnote = "both dose-response and dose response terms were searched for"
     )%>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "benchmark_dose",
     footnote = "both benchmark-dose and benchmark dose terms were searched for"
     )%>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "modelling",
     footnote = "both modelling and modeling terms were searched for"
     )%>%
 modify_caption("**Table 5. Distribution of retrieved words by source document**")
tbl5


tbl6 <- 
  jecfa_TM %>% 
  mutate(dose_response = dose.response | dose.response.1, 
         modelling = modelling | modeling, 
         benchmark_dose = benchmark.dose | benchmark.dose.1,
         bmd = case_when(bmd == 0 ~"bmd no", bmd == 1 ~"bmd yes")) %>% 
#  filter(source_TM == "FAS") %>%
  select(dose_response, 
         modelling, 
         bmd, 
         bmr, 
         benchmark_dose,
         source_TM) %>% 

  tbl_strata(
    strata = source_TM,
    ~.x %>%
 
 tbl_summary(
    missing = "no",
    percent = "col",
    label = list(dose_response ~ "dose response", benchmark_dose ~ "benchmark dose"),sort = all_categorical() ~ "alphanumeric",
    by = bmd
  ) %>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "dose_response",
     footnote = "both dose-response and dose response terms were searched for"
     )%>%
      
  modify_table_styling(
    columns = label,
     rows = variable == "benchmark_dose",
     footnote = "both benchmark-dose and benchmark dose terms were searched for"
     )%>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "modelling",
     footnote = "both modelling and modeling terms were searched for"
     )%>%
  
  modify_caption("**Table 6. Distribution by text bmd and document source**")  
)
tbl6


tbl7 <- 
  jecfa_TM %>% 
  mutate(dose_response = dose.response | dose.response.1, 
         modelling = modelling | modeling, 
         benchmark_dose = benchmark.dose | benchmark.dose.1) %>% 
  mutate(dose_response = case_when(dose_response == F ~"dose-re no", dose_response == T ~"dose-re yes")) %>% 
#  filter(source_TM == "FAS") %>%
  select(dose_response, 
         modelling, 
         bmd, 
         bmr, 
         benchmark_dose,
         source_TM) %>% 
tbl_strata(
    strata = source_TM,
    ~.x %>%
      
    tbl_summary(
    missing = "no",
    percent = "col",
    label = list(dose_response ~ "dose response", benchmark_dose ~ "benchmark dose"),sort = all_categorical() ~ "alphanumeric",
    by = dose_response
  ) %>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "dose_response",
     footnote = "both dose-response and dose response terms were searched for"
     )%>%
      
  modify_table_styling(
    columns = label,
     rows = variable == "benchmark_dose",
     footnote = "both benchmark-dose and benchmark dose terms were searched for"
     )%>%
  
  modify_table_styling(
    columns = label,
     rows = variable == "modelling",
     footnote = "both modelling and modeling terms were searched for"
     )%>%
  
  modify_caption("**Table 7. Distribution by text dose response and document source**")  
)

tbl7



```

```{r, eval=T, cache=F, echo=F, warning=FALSE, message=FALSE}

maptoJecfa_all<- readRDS("~/Efsa_pdf_tm/Jecfa_web/maptoJecfa_all.rds")


tbl8 <- 
      maptoJecfa_all %>% 
      mutate(source_tm = ifelse(is.na(source_TM) == TRUE,"Unknown",source_TM), 
             Monograph = ifelse(is.na(Tox_monograph_abbr) == TRUE, "no",ifelse(substr(Tox_monograph_abbr,1, 12) == "NOT PREPARED", "no", "yes")), 
             Report = ifelse(Report == "","no","yes")) %>%
      select(source_tm, Monograph, Report) %>% 
 
      tbl_summary( missing = "no",
                   percent = "col",
                   by = source_tm,
             #     label = list(dose_response ~ "dose response", benchmark_dose ~ "benchmark dose"),
                   sort = all_categorical() ~ "alphanumeric"
    
                   )%>%
#   add_overall()%>%
modify_caption("**Table 8. Distribution of retrieved documents**")  

tbl8

tbl9 <- 
      maptoJecfa_all %>% 
      mutate(source_tm = if_else(source_TM == "0","Unknown",source_TM),
             unique1 = !duplicated(cbind(Functional.Class, CAS.number, Evaluation.year, COE.number, FEMA.number)),
             unique2 = !duplicated(cbind(Functional.Class, CAS.number, Evaluation.year, COE.number)),
             unique3 = !duplicated(cbind(Functional.Class, CAS.number, Evaluation.year)),
             unique4 = !duplicated(cbind(CAS.number, Evaluation.year))) %>%
select(unique1, unique2, unique3, unique4, source_tm) %>%
      tbl_summary( missing = "no",
                   missing_text = "Unknown",
                   percent = "col",
                   by = source_tm,
                   type = list(c(unique1, unique2, unique3, unique4) ~ "categorical")
                   )%>%
  modify_table_styling(
    columns = label,
     rows = variable == "unique1",
     footnote = "unique combination of Functional.Class, CAS.number, Evaluation.year, COE.number, FEMA.number"
     )%>%
  modify_table_styling(
    columns = label,
     rows = variable == "unique2",
     footnote = "unique combination of Functional.Class, CAS.number, Evaluation.year, COE.number"
     )%>%
  modify_table_styling(
    columns = label,
     rows = variable == "unique3",
     footnote = "unique combination of Functional.Class, CAS.number, Evaluation.year"
     )%>%
  add_overall()%>%
  modify_table_styling(
    columns = label,
     rows = variable == "unique4",
     footnote = "unique combination of CAS.number, Evaluation.year"
     )%>%
    remove_row_type(variables = c(unique1, unique2, unique3, unique4), 
                  type = "level", level_value = ("FALSE"))%>%
modify_caption("**Table 9. Distribution of unique records depending on the set of variables considered**")  

tbl9


```

## Filtering of jecfa records by bmd, bmr or benchmark dose and identify unique records (N= 1122)
```{r, eval=T, cache=F, echo=F, warning=FALSE, message=FALSE}

maptoJecfa_all<- readRDS("~/Efsa_pdf_tm/Jecfa_web/maptoJecfa_all.rds")

# Convert page matches with a 1/0 variable
maptoJecfa_all[, 23:30]<-as.integer(maptoJecfa_all[, 23:30] != 0)
maptoJecfa_all[is.na(maptoJecfa_all)] <- 0

tbl10 <- 
      maptoJecfa_all %>%
      filter (bmd == 1 |
             bmr == 1 |
             benchmark.dose == 1 | 
             benchmark.dose.1 == 1) %>%
      mutate(Some_id = if_else(CAS.number =="0" & COE.number=="0" & FEMA.number=="0",      "Unknown",">=1 Present"),
             unique1 = !duplicated(cbind(Functional.Class,    CAS.number,Evaluation.year,COE.number,FEMA.number))) %>%
select(Functional.Class, CAS.number, Some_id, COE.number, FEMA.number, unique1) %>%
mutate(across(
    c(Functional.Class, CAS.number, COE.number, FEMA.number),
    ~ case_when(
      . == "0" ~ FALSE,
      TRUE ~ TRUE
    ),
    .names = "{.col} (non_missing)"),
    .keep ="unused") %>%
      tbl_summary( 
        #missing = "always",
                   percent = "col",
                   by = Some_id,
                   type = list(c(unique1) ~ "categorical")
                   )%>%
  modify_table_styling(
    columns = label,
     rows = variable == "unique1",
     footnote = "unique combination of Functional.Class, CAS.number, Evaluation.year, COE.number, FEMA.number"
     )%>%
  add_overall()%>%
modify_caption("**Table 10. Distribution of identifiers when at least one among CAS, COE and FEMA is present**")  

tbl10

maptoJecfa_filter <- maptoJecfa_all %>%
     filter (bmd == 1 |
             bmr == 1 |
             benchmark.dose == 1 | 
             benchmark.dose.1 == 1) %>%
      group_by(Functional.Class, 
             CAS.number, 
             Evaluation.year, 
             COE.number, 
             FEMA.number) %>%
      mutate(Chemical.Names_conc = paste0(Chemical.Names, collapse = ", "),
             URL_conc = paste0(URL, collapse = ", "))%>%
    distinct(Functional.Class, 
             CAS.number, 
             Evaluation.year, 
             COE.number, 
             FEMA.number, 
             .keep_all = TRUE) 

# Sort by File for ease of consultation
maptoJecfa_filter <- maptoJecfa_filter[order(maptoJecfa_filter$File), ]

# Create a new identifier for import into DistillerSR
maptoJecfa_filter$RefID_Distiller <- 1:nrow(maptoJecfa_filter)


maptoJecfa_filter$CAS.number_1 <- substr(maptoJecfa_filter$CAS.number,3, nchar(maptoJecfa_filter$CAS.number)-2)

JecfatoDistiller<- merge(maptoJecfa_filter[,c(1:22,32:34)], jecfa_TM[,c(-1,-11)], by="RefID", all.x = T)

# write.csv2(JecfatoDistiller, "JecfatoDistillerIII.csv") 
```

## Retrive pdf to upload into DistillerSR

```{r, eval=F, cache=F, echo=F, warning=FALSE, message=FALSE}

# List of filenames from JecfatoDistiller$File 
jecfato_file_list <- unique(JecfatoDistiller$File)

# Directories containing the source PDF files
source_directories <- c("TRS_Unique", file.path("FAS", "jecfa_files"))

# Directory where the PDF files will be stored
output_directory <- "PDF_JecfatoDistiller"

# Create the output directory if it doesn't exist
if (!dir.exists(output_directory)) {
  dir.create(output_directory)
}

# Iterate through the source directories
for (source_directory in source_directories) {
  files <- list.files(path = source_directory, full.names = TRUE)
  matching_files <- files[basename(files) %in% jecfato_file_list]

  for (matching_file in matching_files) {
    destination_file_path <- file.path(output_directory, basename(matching_file))
    file.copy(matching_file, destination_file_path)
    cat("Copying:", matching_file, "to", destination_file_path, "\n")
  }
}

cat("File copying complete.\n")
